{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorized Variational Autoencoder\n",
    "\n",
    "Following the model specified in the Disney Research paper \"Factorized Variational Autoencoders for\n",
    "Modeling Audience Reactions to Movies\"\n",
    "\n",
    "## Testing with toy data\n",
    "We begin by testing with a toy dataset taken from the Edward examples:\n",
    "https://github.com/blei-lab/edward/blob/master/examples/probabilistic_matrix_factorization.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfd = tf.contrib.distributions\n",
    "\n",
    "N = 1000\n",
    "M = 10000\n",
    "D = 3\n",
    "sigma = 0.1\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "\n",
    "U_true = np.random.randn(D, N)\n",
    "V_true = np.random.randn(D, M)\n",
    "\n",
    "X = np.dot(np.transpose(U_true), V_true) + np.random.normal(0, sigma, size=(N, M))\n",
    "X = X.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder(data, z_dim, batch_size, num_features):\n",
    "    data = tf.reshape(data, [batch_size, num_features])\n",
    "\n",
    "    # sample latent variables\n",
    "    x = tf.layers.dense(inputs=data,\n",
    "            units=512, activation=tf.nn.relu)\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "            units=256, activation=tf.nn.relu)\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "            units=128, activation=tf.nn.relu)\n",
    "    u_net = tf.layers.dense(inputs=x,\n",
    "                      units = z_dim * 2,\n",
    "                      activation=None)\n",
    "    u_loc = u_net[..., :z_dim]\n",
    "    u_scale = u_net[..., z_dim:]\n",
    "    u = tfd.MultivariateNormalDiag(u_loc, scale_diag=u_scale, name='sample_latent_U')\n",
    "    \n",
    "    # observation latent variables\n",
    "    x_t = tf.transpose(data)\n",
    "    x_t = tf.layers.dense(inputs=x_t,\n",
    "            units=64, activation=tf.nn.relu)\n",
    "    x_t = tf.layers.dense(inputs=x_t,\n",
    "            units=32, activation=tf.nn.relu)\n",
    "    x_t = tf.layers.dense(inputs=x_t,\n",
    "            units=16, activation=tf.nn.relu)\n",
    "    v_net = tf.layers.dense(inputs=x_t,\n",
    "                      units = z_dim * 2,\n",
    "                      activation=None)\n",
    "    v_loc = v_net[..., z_dim:]    \n",
    "    v_scale = v_net[..., :z_dim]\n",
    "    \n",
    "    v = tfd.MultivariateNormalDiag(v_loc, scale_diag=v_scale, name='observation_latent_V')\n",
    "\n",
    "    print(u.sample())\n",
    "    print(v.sample())\n",
    "    \n",
    "    # factorized latent variables\n",
    "    z = tfd.MultivariateNormalDiag(loc=tf.multiply(u.sample(), v.sample()),\n",
    "            scale_diag=tf.ones(z_dim),\n",
    "            name='approximate_posterior_q')\n",
    " \n",
    "    return u, v, z\n",
    "\n",
    "\n",
    "def make_decoder(z, num_features, z_dim):\n",
    "    x = tf.layers.dense(inputs=z,\n",
    "            units=128, activation=tf.nn.relu)\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "             units=256, activation=tf.nn.relu)\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "            units=512, activation=tf.nn.relu)\n",
    "    \n",
    "    decoder_net = tf.layers.dense(inputs=x,\n",
    "            units=num_features, activation=None)\n",
    "    \n",
    "    data_dist = tfd.MultivariateNormalDiag(loc=decoder_net,\n",
    "                            name='posterior_p')\n",
    "        \n",
    "    return data_dist\n",
    "\n",
    "\n",
    "def make_prior(z_dim):\n",
    "    u_prior =  tfd.MultivariateNormalDiag(scale_diag=tf.ones(z_dim),\n",
    "                                    name='U')\n",
    "    v_prior = tfd.MultivariateNormalDiag(scale_diag=tf.ones(z_dim),\n",
    "                                    name='V')\n",
    "        \n",
    "    return u_prior, v_prior\n",
    "\n",
    "\n",
    "def prior_prob(u_prior, v_prior):\n",
    "    loc = tf.multiply(u_prior.sample(), v_prior.sample())\n",
    "    \n",
    "    return tfd.MultivariateNormalDiag(loc=loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder/sample_latent_U/sample/affine_linear_operator/forward/add:0\", shape=(100, 3), dtype=float32)\n",
      "Tensor(\"encoder/observation_latent_V/sample/affine_linear_operator/forward/add:0\", shape=(10000, 3), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 100 and 10000 for 'encoder/Mul' (op: 'Mul') with input shapes: [100,3], [10000,3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 100 and 10000 for 'encoder/Mul' (op: 'Mul') with input shapes: [100,3], [10000,3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5f9261671bc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         u, v, encoder_q = make_encoder(data, z_dim=D, batch_size=batch_size,\n\u001b[0;32m---> 15\u001b[0;31m                               num_features=M)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-49d4a3220177>\u001b[0m in \u001b[0;36mmake_encoder\u001b[0;34m(data, z_dim, batch_size, num_features)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# factorized latent variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     z = tfd.MultivariateNormalDiag(loc=tf.multiply(u.sample(), v.sample()),\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mscale_diag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             name='approximate_posterior_q')\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multiply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   4757\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4758\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 4759\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   4760\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4761\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3415\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3417\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1757\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1758\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1759\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 100 and 10000 for 'encoder/Mul' (op: 'Mul') with input shapes: [100,3], [10000,3]."
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input pipeline\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    data = iterator.get_next()\n",
    "    \n",
    "    with tf.variable_scope('priors'):\n",
    "        U_prior, V_prior = make_prior(z_dim=D)\n",
    "        \n",
    "    # inference network; encoder\n",
    "    with tf.variable_scope('encoder'):\n",
    "        u, v, encoder_q = make_encoder(data, z_dim=D, batch_size=batch_size,\n",
    "                              num_features=M)\n",
    "    \n",
    "    z = encoder_q.sample()\n",
    "    u_hat = u.mean()\n",
    "    v_hat = v.mean()\n",
    "\n",
    "    # generative network; decoder\n",
    "    with tf.variable_scope('decoder'):\n",
    "        decoder_p = make_decoder(z, z_dim=D, num_features=M)\n",
    "    \n",
    "    # prior\n",
    "    with tf.variable_scope('prior'):\n",
    "        u_prior, v_prior = make_prior(z_dim=D)\n",
    "\n",
    "    # loss\n",
    "    log_p_v = v.log_prob(v.mean())\n",
    "    log_p_u = u.log_prob(u.mean())\n",
    "    likelihood = decoder_p.log_prob(data)\n",
    "    kl_divergence = tfd.kl_divergence(encoder_q, prior_prob(u_prior, v_prior))\n",
    "    loss = tf.reduce_mean(kl_divergence - likelihood + log_p_u + log_p_v)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate parameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "import datetime\n",
    "run = 'run-{date:%d.%m.%Y_%H:%M:%S}'.format( date=datetime.datetime.now() )\n",
    "tb_writer = tf.summary.FileWriter('/logs/' + run, graph=graph)\n",
    "\n",
    "# training\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                _, epoch_z, summary = sess.run([optimizer, z, merged])\n",
    "                tb_writer.add_summary(summary, batch_counter)\n",
    "                batch_counter += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see how well we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
