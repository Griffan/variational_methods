{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genotype Factorized Variational Autoencoder\n",
    "\n",
    "Putting together our various strains of work for a toy factorized variational autoencoder based on genotypes. The idea is to get the basic framework running, with the aim of getting some sensible results during training (i.e. can we maximize the evidence lower bound)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pandas_plink import read_plink\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "tfd = tf.contrib.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model and analysis parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # samples\n",
    "M = 1000 # sites\n",
    "D = 2 # latent dimension\n",
    "\n",
    "batch_size = N\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup some `io` related things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tfrecords(tfrecords_filename, m_variants):\n",
    "    '''\n",
    "    Parse a tf.string pointing to *.tfrecords into a genotype tensor,  rows: variants, cols: samples)\n",
    "    Helpful blog post:\n",
    "    http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/\n",
    "    '''\n",
    "    data = tf.parse_example([tfrecords_filename],\n",
    "        {'genotypes': tf.FixedLenFeature([], tf.string)})\n",
    "\n",
    "    gene_vector = tf.decode_raw(data['genotypes'], tf.int8)\n",
    "    gene_vector = tf.reshape(gene_vector, [1, m_variants])\n",
    "\n",
    "    return gene_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions for model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder(data, z_dim, batch_size, num_features):\n",
    "    data = tf.reshape(data, [batch_size, num_features])\n",
    "\n",
    "    # sample latent variables\n",
    "    x = tf.layers.dense(inputs=data,\n",
    "            units=64, activation=tf.nn.sigmoid)\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "            units=32, activation=tf.nn.sigmoid)\n",
    "    x = tf.layers.dense(inputs=x,\n",
    "            units=128, activation=tf.nn.sigmoid)\n",
    "    u_net = tf.layers.dense(inputs=x,\n",
    "                      units = z_dim * 2,\n",
    "                      activation=None)\n",
    "    u_loc = u_net[..., :z_dim]\n",
    "    u_scale = tf.nn.softplus(u_net[..., z_dim:] + 0.5)\n",
    "    u = tfd.MultivariateNormalDiag(u_loc, scale_diag=u_scale,\n",
    "                                   name='sample_latent_U')\n",
    "    \n",
    "    # observation latent variables\n",
    "    x_t = tf.transpose(data)\n",
    "    x_t = tf.layers.dense(inputs=x_t,\n",
    "            units=64, activation=tf.nn.sigmoid)\n",
    "    x_t = tf.layers.dense(inputs=x_t,\n",
    "            units=32, activation=tf.nn.sigmoid)\n",
    "    x_t = tf.layers.dense(inputs=x_t,\n",
    "            units=16, activation=tf.nn.sigmoid)\n",
    "    v_net = tf.layers.dense(inputs=x_t,\n",
    "                      units = z_dim * 2,\n",
    "                      activation=None)\n",
    "    v_loc = v_net[..., z_dim:]    \n",
    "    v_scale = tf.nn.softplus(v_net[..., :z_dim] + 0.5)\n",
    "    \n",
    "    v = tfd.MultivariateNormalDiag(v_loc, scale_diag=v_scale,\n",
    "                                   name='observation_latent_V')\n",
    "    \n",
    "    return u, v\n",
    "\n",
    "\n",
    "def make_decoder(u, v, batch_size, num_features, z_dim):\n",
    "    \n",
    "    # \"dot product decoder\"\n",
    "    z = tf.tensordot(u, v, axes=[[1], [1]])\n",
    "    z = tf.reshape(z, [1, num_features*batch_size])\n",
    "    logits = tf.nn.softplus(z)\n",
    "    \n",
    "    # assume fixed, unit variance\n",
    "    data_dist = tfd.Independent(tfd.Binomial(logits=logits, total_count=2.0),\n",
    "                    reinterpreted_batch_ndims=1,\n",
    "                    name='posterior_p')\n",
    "        \n",
    "    return data_dist\n",
    "\n",
    "\n",
    "def make_prior(z_dim):\n",
    "    u_prior =  tfd.MultivariateNormalDiag(scale_diag=tf.ones(z_dim),\n",
    "                                    name='U')\n",
    "    v_prior = tfd.MultivariateNormalDiag(scale_diag=tf.ones(z_dim),\n",
    "                                    name='V')\n",
    "\n",
    "    return u_prior, v_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and input pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input pipeline\n",
    "    dataset = tf.data.TFRecordDataset('data/test.tfrecords', compression_type=tf.constant('ZLIB'))\n",
    "    dataset = dataset.map(lambda fn: decode_tfrecords(fn, M))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    data = iterator.get_next()\n",
    "    data = tf.cast(data, tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('priors'):\n",
    "        u_prior, u_prior = make_prior(z_dim=D)\n",
    "        \n",
    "    # inference network; encoder\n",
    "    with tf.variable_scope('encoder'):\n",
    "        u_encoder, v_encoder = make_encoder(data, z_dim=D,\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_features=M)\n",
    "    \n",
    "    u = u_encoder.sample()\n",
    "    v = v_encoder.sample()\n",
    "\n",
    "    # generative network; decoder\n",
    "    with tf.variable_scope('decoder'):\n",
    "        decoder_p = make_decoder(u, v, z_dim=D, num_features=M,\n",
    "                                 batch_size=batch_size)\n",
    "    \n",
    "    # prior\n",
    "    with tf.variable_scope('prior'):\n",
    "        u_prior, v_prior = make_prior(z_dim=D)\n",
    "\n",
    "    # loss\n",
    "    u_kl = tf.reduce_sum(tfd.kl_divergence(u_encoder, u_prior))\n",
    "    v_kl = tf.reduce_sum(tfd.kl_divergence(v_encoder, v_prior))\n",
    "    likelihood = tf.reduce_sum(decoder_p.log_prob(tf.reshape(data, [1, N*M])))\n",
    "    elbo = -u_kl - v_kl + likelihood\n",
    "    tf.summary.scalar('elbo', elbo)\n",
    "    tf.summary.scalar('minus_u_kl', tf.negative(u_kl))\n",
    "    tf.summary.scalar('minus_v_kl', tf.negative(v_kl))\n",
    "    tf.summary.scalar('likelihood', likelihood)\n",
    "\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(-elbo)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate model parameters and monitor the routine in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100976.47\n",
      "-100537.88\n",
      "-97555.02\n",
      "-96816.87\n",
      "-96311.48\n",
      "-95627.76\n",
      "-94476.54\n",
      "-94997.836\n",
      "-95082.195\n",
      "-94932.6\n"
     ]
    }
   ],
   "source": [
    "# tensorboard\n",
    "run = 'run-{date:%d.%m.%Y_%H:%M:%S}'.format(date=datetime.datetime.now())\n",
    "tb_writer = tf.summary.FileWriter('/logs/geno_fvae/' + run, graph=graph)\n",
    "\n",
    "# training\n",
    "with tf.Session(graph=graph) as sess:    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        while True:\n",
    "            try:\n",
    "                _, tb_summary, epoch_elbo = sess.run([optimizer, merged, elbo])\n",
    "                print(epoch_elbo)\n",
    "                tb_writer.add_summary(tb_summary, epoch)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
